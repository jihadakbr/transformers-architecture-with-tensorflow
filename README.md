
# Transformers Architecture with TensorFlow

This project was completed as a part of the Honors portion of the [Sequence Models](https://www.coursera.org/learn/nlp-sequence-models) Course on [Coursera](https://www.coursera.org/).

Credit to DeepLearning.AI and the Coursera platform for providing the course materials and guidance.

## Objective

In this notebook, my primary objective is to explore and understand the Transformer architecture, a sophisticated neural network designed to take advantage of parallel processing, leading to significant improvements in the training process. Throughout this assignment, I will gain valuable insights into various aspects of the Transformer architecture. Firstly, I will learn how to create positional encodings, which play a crucial role in capturing sequential relationships within the data. This will allow the model to better understand the order and context of input sequences. 

Next, I will delve into the concept of scaled dot-product self-attention, which involves calculating attention weights between different words in a sentence. This mechanism helps the model focus on relevant words and learn meaningful representations. Additionally, I will implement masked multi-head attention, a fundamental building block of the Transformer. This allows the model to selectively attend to different parts of the input, enabling it to handle variable-length sequences efficiently. Finally, I will build and train a complete Transformer model, harnessing its parallel processing capabilities to expedite the training process and achieve improved performance.
## Results

![Transformers Architecture with TensorFlow](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhphtC20975N7B7hbgy_ciEn6U8eazfCE0UdonyeNX1bI2Y5DpfBeHkN8MLGjPPaJK9zpzkHdnwC-70ZLX_ZozkVWnJFWdoS-Fqxe-L0kBECWkWGtFGKMQaMCMuaabSJl4m_xzSn3rGLuxFxOPLu9wZbZj27PM708VnrQP3wEKSc4hy1v3_PxSyT4bY3x4/s1600/transformers-architecture-with-tensorflow.png)